# -*- coding: utf-8 -*-
"""Russia Misinformation_updated September 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eRSxu5-opvXWx30VXoK1a3cHSvv-WKbU

For word Embeddings using Russia Misinformation twitter dataset
"""

## Load Packages ##
import os
import re
from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel
from collections import defaultdict
import torch
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import nltk
from nltk.corpus import stopwords
from google.colab import userdata
nltk.download('stopwords')
import requests
from transformers import DistilBertModel, DistilBertTokenizer
import torch

from google.colab import drive
drive.mount('/content/drive/')

### Initialize Model and Stopword List and Load Dataset ###
#download stopwords
nltk.download('stopwords')
stopwords_eng = set(stopwords.words("english"))
more_words = {"http", "https", "rt", "co", "lol", "also", "yeah"}
stopwords_eng.update(more_words)
stopwords_list = requests.get("https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt").content
more_words = set(stopwords_list.decode().splitlines())
stopwords_eng.update(more_words)

## Use all-MiniLM-L6-v2, 9/5
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
model.eval()

file_path = '/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_after_48hrs_text_llcleaned.txt' ## use Lingyao's russia_before_48hrs_text_llcleaned

with open(file_path, 'r', encoding='utf-8') as file:
    combined_text = file.read()

combined_text = [combined_text]

### Run Bert and Display Results## add 8/14: change the cosine cutoff

# Tokenize text
inputs = tokenizer(combined_text, return_tensors="pt", padding=True, truncation=True, max_length=512, add_special_tokens=True)
with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.squeeze(0)

# Define key term
key_terms = ['Russia']  ## terms needed: History, Truth, NATO, Putin, Russia, Ukraine  ## 10/4
#key_terms = ['Crimea', 'Donbas', 'Lenin', 'NATO', 'Ukraine', 'Russia', 'Poland', 'Fascist', 'WWII']

# Token to word mapping
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
words = []
for token in tokens:
        if token.startswith("##"):
              words[-1] += token[2:]  # Append subword to the last word
        else:
            words.append(token)

for key_term in key_terms:
    # Get embedding for the key term
      key_term_inputs = tokenizer(key_term, return_tensors="pt", add_special_tokens=True)
      with torch.no_grad():
          key_term_output = model(**key_term_inputs)
          key_term_embedding = key_term_output.last_hidden_state.mean(dim=1)  # Key term embedding

    # Calculate cosine similarities
similarities = cosine_similarity(embeddings.detach().numpy(), key_term_embedding.detach().numpy().reshape(1, -1)).flatten()

#remove stopwords
filtered_words = [word for word in words if word.isalpha() and word.lower() not in stopwords_eng]
filtered_input_ids = tokenizer.convert_tokens_to_ids(filtered_words)
filtered_inputs = {"input_ids": torch.tensor([filtered_input_ids])}

    # Pair words with their similarity scores, removing duplicates and non-alphabetic tokens
word_similarity_pairs = list(dict.fromkeys(zip(filtered_words, similarities)))  # Remove duplicates
filtered_pairs = [(word, sim) for word, sim in word_similarity_pairs if word.isalpha() and word.lower() != key_term.lower()]

    # Sort by similarity score
sorted_pairs = sorted(filtered_pairs, key=lambda x: x[1], reverse=True)[:40] ## top n words here

    #create filtered emebddings for cosmo graph
similarity_threshold = 0.5 ##change to 0.05 8/14, and 0.1 in 8/17

filtered_embeddings = []
for word, similarity in zip(filtered_words, similarities):
    if similarity > similarity_threshold:
        filtered_embeddings.append(embeddings[words.index(word)].numpy())

    # Convert the filtered embeddings
filtered_embeddings = torch.tensor(filtered_embeddings)

### Post-processing, new 8/11 code, 8/14
# Remove words with 2 or fewer characters
filtered_terms = [(word, sim) for word, sim in sorted_pairs if len(word) > 2]

# Display results
print(f"Top N words closest to '{key_term}':")
for word, similarity in filtered_terms:
    print(f"{word}: {similarity:.4f}")
    print("\n")

#visualize embeddings

tsne = TSNE(n_components = 2, random_state = 42)
tsne_embeddings = tsne.fit_transform(embeddings)

#highlight words to appear on embeddings graph
highlighted_words = ["bombs","pipeline","border","association","politically","china","journalists","rt","jurgen","right",
                     "captivity","countries","public","path","conflict","sabotage","soon","germany","eu","support","muellershewrote"]

plt.figure(figsize=(10, 8))
plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c='b', marker='o')
for i, word in enumerate(words):
    if word in highlighted_words:
        plt.annotate(word, xy=(tsne_embeddings[i, 0], tsne_embeddings[i, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom', fontsize=10, color='red')
plt.title('BERT Word Embeddings Visualized with t-SNE (WWII)')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid(True)
plt.show()

#filter words and create new tsne_embeddings
tsne_embeddings_filtered = tsne.fit_transform(filtered_embeddings)



import csv
## get csv to run in cosmograph
cosmog = f'drive/MyDrive/Diego_Colab Notebooks/Embeddings/Cosmograph/All POS/{key_term}_embedding_filtered.csv'

# Write the data to the CSV file
with open(cosmog, mode='w', newline='') as file:
    writer = csv.writer(file, delimiter=';')
    writer.writerow(['id', 'x', 'y'])
    for word, (x, y) in zip(words, tsne_embeddings_filtered):
        writer.writerow([word, x, y])

## read in and prep data for TFIDF ##
data1 = pd.read_csv('/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_before_48hrs.csv', encoding = 'latin1')
data2 = pd.read_csv('/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_after_48hrs.csv', encoding = 'latin1')

# write to text file
data1[['Full Text']].to_csv('/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_before_48hrs.txt', sep='\t', index=False)
data2[['Full Text']].to_csv('/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_after_48hrs.txt', sep='\t', index=False)

data1.head(n=5)

### TFIDF, AFTER INTERVIEW ###

file_path = '/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_after_new.txt'

stopwords_2 = set(stopwords.words('english')) | {"http","https","rt","co", "would"}
stopwords_2 = list(stopwords_2)
print(stopwords_2)
# Read the text file
with open(file_path, 'r', encoding='utf-8') as file:
    document = file.read()

documents = [document]

# Create a TF-IDF Vectorizer
tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words = stopwords_2)

# Fit and transform the documents
tfidf_wm = tfidfvectorizer.fit_transform(documents)

# Retrieve the terms found in the corpus
tfidf_tokens = tfidfvectorizer.get_feature_names_out()

# Create a DataFrame with the results
df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Document'], columns = tfidf_tokens)

# Transpose the DataFrame to have terms as rows
df_tfidfvect = df_tfidfvect.T

# Sort the terms by their TF-IDF score
df_tfidfvect_sorted_after = df_tfidfvect.sort_values(by='Document', ascending=False)

# Print the top N terms
print(df_tfidfvect_sorted_after.head(20))

### TFIDF, BEFORE INTERVIEW ###

file_path = '/content/drive/MyDrive/Diego_Colab Notebooks/Data/russia_before_new.txt'

stopwords_2 = set(stopwords.words('english')) | {"http","https","rt","co", "would"}
stopwords_2 = list(stopwords_2)
print(stopwords_2)
# Read the text file
with open(file_path, 'r', encoding='utf-8') as file:
    document = file.read()

documents = [document]

# Create a TF-IDF Vectorizer
tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words = stopwords_2)

# Fit and transform the documents
tfidf_wm = tfidfvectorizer.fit_transform(documents)

# Retrieve the terms found in the corpus
tfidf_tokens = tfidfvectorizer.get_feature_names_out()

# Create a DataFrame with the results
df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Document'], columns = tfidf_tokens)

# Transpose the DataFrame to have terms as rows
df_tfidfvect = df_tfidfvect.T

# Sort the terms by their TF-IDF score
df_tfidfvect_sorted_before = df_tfidfvect.sort_values(by='Document', ascending=False)

# Print the top N terms
print(df_tfidfvect_sorted_before.head(20))

## check if normally distributed
from scipy.stats import shapiro

stat, p_value = shapiro(df_tfidfvect_sorted_before)

print(f"Shapiro-Wilk Test Statistic: {stat}")
print(f"P-Value: {p_value}")

if p_value > 0.05:
    print("Data is normally distributed (fail to reject H0).")
else:
    print("Data is not normally distributed (reject H0).")

## Wilcoxon signed-rank test
from scipy.stats import wilcoxon

# Get both DataFrames to have the same index (terms)
common_terms = df_tfidfvect_sorted_before.index.intersection(df_tfidfvect_sorted_after.index)
df_before_common = df_tfidfvect_sorted_before.loc[common_terms]
df_after_common = df_tfidfvect_sorted_after.loc[common_terms]

# Sort by TF-IDF values and get the top 20 terms
top_terms_before = df_before_common['Document'].sort_values(ascending=False).head(20)
top_terms_after = df_after_common.loc[top_terms_before.index]

# Extract TF-IDF values for the top 20 terms
before_values = top_terms_before.values
after_values = top_terms_after['Document'].values

# Perform the Wilcoxon signed-rank test
stat, p_value = wilcoxon(before_values, after_values)

# Store the results in a DataFrame
df_results = pd.DataFrame({
    'Term': top_terms_before.index,
    'Before TF-IDF': before_values,
    'After TF-IDF': after_values,
    'Difference': after_values - before_values
})

# Add the Wilcoxon test statistic and p-value as additional columns
df_results['Statistic'] = round(stat, 2)
df_results['P-Value'] = round(p_value, 2)

# Display the results
print(df_results)

# Save the results to a CSV file
df_results.to_csv('wilcoxon_before_after_terms.csv', index=False)
